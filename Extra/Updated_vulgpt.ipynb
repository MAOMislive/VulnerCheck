{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFrdBtflg3iD",
        "outputId": "cb68f9f6-2b45-43bb-d846-9cd0b70d1d93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDBTHYHhg3iG"
      },
      "source": [
        "****What is Vulngpt ?\n",
        "-** This an application where user can Detect the Basic website Vulnerbility.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtIsiqIXg3iI"
      },
      "source": [
        "# Step 1: Install and Import Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2asARvczg3iJ",
        "outputId": "22880686-1d13-4472-b4ad-b03b02a3404d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "\n",
        "import pandas as pd\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ6EMCz2g3iK"
      },
      "source": [
        "# **Step2: Load SQL Injection and Other Vulnerabilities Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb5WM0Iui58Q",
        "outputId": "5592df14-8ad4-42c6-e720-0a0e8ae2f34c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SQL Injection Dataset:\n",
            "      ID                                          SQL Query Injection Payload  \\\n",
            "521  522         UPDATE accounts SET balance=500 WHERE id=1               NaN   \n",
            "737  738  DELETE FROM orders WHERE order_id = john.doe@e...               NaN   \n",
            "740  741         UPDATE accounts SET balance=500 WHERE id=2               NaN   \n",
            "660  661     UPDATE accounts SET balance=500 WHERE id=apple               NaN   \n",
            "411  412  SELECT email FROM customers WHERE email = '' O...           1=1;--'   \n",
            "\n",
            "    Injection Type Vulnerability Status     Injection Result Attack Type  \n",
            "521            NaN                   No                  NaN      Normal  \n",
            "737            NaN                   No                  NaN      Normal  \n",
            "740            NaN                   No                  NaN      Normal  \n",
            "660            NaN                   No                  NaN      Normal  \n",
            "411    Union-based                  Yes  Unauthorized access      Attack  \n",
            "\n",
            "Other Vulnerabilities Dataset:\n",
            "      ID Attack Type                          Attack Vector  \\\n",
            "93    94         RCE    POST request with command injection   \n",
            "692  693         RCE    POST request with command injection   \n",
            "100  101         RCE                                ?cmd=ls   \n",
            "562  563        CSRF  Forged POST request to transfer funds   \n",
            "729  730        CSRF  Forged POST request to transfer funds   \n",
            "\n",
            "    Vulnerability Status                          Attack Result Request Type  \\\n",
            "93                   Yes               Arbitrary code execution       DELETE   \n",
            "692                  Yes               Arbitrary code execution          GET   \n",
            "100                  Yes               Arbitrary code execution         POST   \n",
            "562                  Yes  Action performed without user consent          PUT   \n",
            "729                  Yes  Action performed without user consent          PUT   \n",
            "\n",
            "                                           Description  \n",
            "93   Remote Code Execution vulnerability through un...  \n",
            "692  Remote Code Execution vulnerability through un...  \n",
            "100  Remote Code Execution vulnerability through un...  \n",
            "562           Cross-Site Request Forgery vulnerability  \n",
            "729           Cross-Site Request Forgery vulnerability  \n",
            "\n",
            "SQL Injection Dataset Columns: Index(['ID', 'SQL Query', 'Injection Payload', 'Injection Type',\n",
            "       'Vulnerability Status', 'Injection Result', 'Attack Type'],\n",
            "      dtype='object')\n",
            "Other Vulnerabilities Dataset Columns: Index(['ID', 'Attack Type', 'Attack Vector', 'Vulnerability Status',\n",
            "       'Attack Result', 'Request Type', 'Description'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Load the SQL Injection dataset\n",
        "sql_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Train/sql_injection2.0.csv')\n",
        "# Load the Other Vulnerabilities dataset (for XSS, CSRF, RCE)\n",
        "other_vuln_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Train/Other_Vulnerabilities_Dataset2.0.csv')\n",
        "\n",
        "# Shuffle the datasets\n",
        "sql_data_shuffled = shuffle(sql_data)\n",
        "other_vuln_data_shuffled = shuffle(other_vuln_data)\n",
        "\n",
        "# Verify the datasets\n",
        "print(\"SQL Injection Dataset:\")\n",
        "print(sql_data_shuffled.head())\n",
        "\n",
        "print(\"\\nOther Vulnerabilities Dataset:\")\n",
        "print(other_vuln_data_shuffled.head())\n",
        "\n",
        "# Print dataset columns\n",
        "print(\"\\nSQL Injection Dataset Columns:\", sql_data_shuffled.columns)\n",
        "print(\"Other Vulnerabilities Dataset Columns:\", other_vuln_data_shuffled.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5EUDONig3iL"
      },
      "source": [
        "# Step 3: Tokenize Both Datasets Using CodeBERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGgMObXGg3iM",
        "outputId": "00e0c071-ef9d-458f-b128-5a4bb49b5c17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SQL Encoded Data:\n",
            "Input IDs: torch.Size([1000, 512])\n",
            "Attention Masks: torch.Size([1000, 512])\n",
            "\n",
            "Other Vulnerabilities Encoded Data:\n",
            "Input IDs: torch.Size([1000, 512])\n",
            "Attention Masks: torch.Size([1000, 512])\n"
          ]
        }
      ],
      "source": [
        "from transformers import RobertaTokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n",
        "\n",
        "# Function to tokenize data from multiple columns and combine input_ids\n",
        "def tokenize_data(dataframe, text_columns):\n",
        "    # Assuming concatenation of texts from different columns for tokenization\n",
        "    concatenated_texts = dataframe[text_columns].apply(lambda x: ' '.join(x.dropna().values), axis=1)\n",
        "    return tokenizer(concatenated_texts.tolist(), padding=\"max_length\", truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "# Columns to tokenize - choose how you concatenate based on your model's capacity to handle input\n",
        "sql_columns = ['SQL Query', 'Vulnerability Status']\n",
        "other_vuln_columns = ['Attack Vector', 'Description']\n",
        "\n",
        "# Tokenize data\n",
        "sql_encoded = tokenize_data(sql_data, sql_columns)\n",
        "other_vuln_encoded = tokenize_data(other_vuln_data, other_vuln_columns)\n",
        "\n",
        "# Printing to verify the outputs\n",
        "print(\"SQL Encoded Data:\")\n",
        "print(f\"Input IDs: {sql_encoded['input_ids'].shape}\")\n",
        "print(f\"Attention Masks: {sql_encoded['attention_mask'].shape}\")\n",
        "\n",
        "print(\"\\nOther Vulnerabilities Encoded Data:\")\n",
        "print(f\"Input IDs: {other_vuln_encoded['input_ids'].shape}\")\n",
        "print(f\"Attention Masks: {other_vuln_encoded['attention_mask'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeAHUfPRg3iN"
      },
      "source": [
        "# Step 5: Create a PyTorch Dataset Class and Split Both Datasets for Training and Validation\n",
        "This class converts the tokenized data into a format compatible with PyTorch’s Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycoF_G4ig8BQ",
        "outputId": "f777c03f-4328-4925-9ff3-6d534a0716d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'input_ids': tensor([[    0, 49179,  1047,  ...,     1,     1,     1],\n",
            "        [    0, 49179,  1009,  ...,     1,     1,     1],\n",
            "        [    0, 49179,  1047,  ...,     1,     1,     1],\n",
            "        ...,\n",
            "        [    0, 10089,  3850,  ...,     1,     1,     1],\n",
            "        [    0, 34543,  2349,  ...,     1,     1,     1],\n",
            "        [    0, 49179,  1009,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        ...,\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1])}\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}  # Correct way to handle tensor copying\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "        #return len(self.labels)\n",
        "\n",
        "# Assuming sql_encoded and other_vuln_encoded are already prepared using your tokenization function\n",
        "class CodeDataset_other(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = [float(label) for label in labels]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Assuming sql_encoded and other_vuln_encoded are already prepared using your tokenization function\n",
        "sql_labels = [1 if x == 'Yes' else 0 for x in sql_data['Vulnerability Status']]\n",
        "other_labels = [1 if x == 'Yes' else 0 for x in other_vuln_data['Vulnerability Status']]\n",
        "\n",
        "# Create dataset instances\n",
        "sql_dataset = CustomDataset(sql_encoded, sql_labels)\n",
        "other_vuln_dataset = CodeDataset_other(other_vuln_encoded, other_labels)\n",
        "\n",
        "# Example: Using DataLoader to handle batches\n",
        "train_loader = DataLoader(sql_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(other_vuln_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Example loop to illustrate how to handle data loader outputs\n",
        "for batch in train_loader:\n",
        "    print(batch)  # This should print the batches without warnings\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "LIt-hfcag3iN",
        "outputId": "90f69834-1e32-4d36-8286-e79cbd793a1a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_test_split' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-68a956d02aaa>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Assuming sql_dataset and other_vuln_dataset are your datasets prepared from previous steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mtrain_sql\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_sql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mtrain_other\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_other\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_vuln_dataset\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Assuming sql_dataset and other_vuln_dataset are your datasets prepared from previous steps\n",
        "train_sql, val_sql = train_test_split(sql_dataset, test_size=0.1, random_state=42)\n",
        "train_other, val_other = train_test_split(other_vuln_dataset , test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfeQyKENg3iP",
        "outputId": "05605b26-2f06-4ab8-b1f8-ff32b5d01e4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vulnerability Type Counts:\n",
            " Attack Type\n",
            "CSRF    273\n",
            "LFI     262\n",
            "XSS     233\n",
            "RCE     232\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# prompt: Show the dataset size of each type in the other_vuln and sql injection csv\n",
        "import pandas as pd # import the pandas library\n",
        "\n",
        "# Load the SQL injection dataset.\n",
        "sql_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Train/sql_injection2.0.csv') # Replace 'path/to/your/sql_injection_dataset.csv' with the actual path\n",
        "\n",
        "# Load the other vulnerabilities dataset\n",
        "other_vuln_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Train/Other_Vulnerabilities_Dataset2.0.csv') # Replace 'path/to/your/other_vulnerabilities_dataset.csv' with the actual path\n",
        "\n",
        "\n",
        "# SQL Injection Dataset:\n",
        "#    ID                                          SQL Query Injection Payload  \\\n",
        "# 0   1  UPDATE accounts SET balance=500 WHERE id=' UNI...               NaN\n",
        "# 1   2               SELECT * FROM users WHERE id = '100'               NaN\n",
        "# 2   3   SELECT * FROM us\n",
        "# Assuming your dataset has a column named 'Vulnerability Type'\n",
        "vulnerability_counts = other_vuln_data['Attack Type'].value_counts()\n",
        "\n",
        "print(\"Vulnerability Type Counts:\\n\", vulnerability_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjviH_XLg3iP"
      },
      "source": [
        "# Step 6: Initialize CodeBERT Models for Both Datasets****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qCw6S3Ng3iQ",
        "outputId": "e1f0eddd-1deb-4eb4-d6a0-bb478e22bc0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "sql_model = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', num_labels=2)\n",
        "other_vuln_model = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', num_labels=other_vuln_data['Vulnerability Status'].nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1bIToyAg3iR"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtFBttq42Epy",
        "outputId": "5f6f7982-529c-4330-bb5a-57bfb92acce9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install seqeval\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IoU Calculation Function"
      ],
      "metadata": {
        "id": "1Cf-womgo-d6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Calculate the Intersection over Union (IoU) of two bounding boxes.\n",
        "\n",
        "    Parameters:\n",
        "    box1 : dict\n",
        "        Keys: {'x1', 'y1', 'x2', 'y2'}\n",
        "        The (x1, y1) position is at the top left corner,\n",
        "        the (x2, y2) position is at the bottom right corner\n",
        "    box2 : dict\n",
        "        Keys: {'x1', 'y1', 'x2', 'y2'}\n",
        "\n",
        "    Returns:\n",
        "    float\n",
        "        in the range [0, 1]\n",
        "    \"\"\"\n",
        "    # Determine the coordinates of the intersection rectangle\n",
        "    x_left = max(box1['x1'], box2['x1'])\n",
        "    y_top = max(box1['y1'], box2['y1'])\n",
        "    x_right = min(box1['x2'], box2['x2'])\n",
        "    y_bottom = min(box1['y2'], box2['y2'])\n",
        "\n",
        "    if x_right < x_left or y_bottom < y_top:\n",
        "        return 0.0\n",
        "\n",
        "    # The area of intersection\n",
        "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
        "\n",
        "    # The area of both AABBs\n",
        "    box1_area = (box1['x2'] - box1['x1']) * (box1['y2'] - box1['y1'])\n",
        "    box2_area = (box2['x2'] - box2['x1']) * (box2['y2'] - box2['y1'])\n",
        "\n",
        "    # Compute the intersection over union by taking the intersection\n",
        "    # area and dividing it by the sum of prediction + ground-truth\n",
        "    # areas - the interesection area.\n",
        "    iou = intersection_area / float(box1_area + box2_area - intersection_area)\n",
        "    return iou\n"
      ],
      "metadata": {
        "id": "qIUQcOC8o7kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# compute_metrics Function"
      ],
      "metadata": {
        "id": "slWDma_FpEt_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D76iN1m-3WiY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=1)  # Assuming you're doing classification\n",
        "\n",
        "    # Placeholder IoU calculation\n",
        "    # You need to ensure that 'labels' and 'predictions' include bounding box info\n",
        "    # This is a stub example assuming box data is available in your labels/predictions\n",
        "    ious = [calculate_iou(pred_box, true_box) for pred_box, true_box in zip(predictions_boxes, labels_boxes)]\n",
        "    mean_iou = sum(ious) / len(ious) if ious else 0\n",
        "\n",
        "    # Calculate precision, recall, and F1-score\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
        "\n",
        "    # Calculate AUC-ROC\n",
        "    auc_roc = roc_auc_score(labels, predictions)\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, predictions),\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1,\n",
        "        \"auc_roc\": auc_roc,\n",
        "        \"mean_iou\": mean_iou  # Include mean IoU in the metrics returned\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juH_KhN22SX1",
        "outputId": "9b636bb8-571b-4432-930e-d9ef6746fc5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import RobertaForSequenceClassification\n",
        "\n",
        "# Define sql_model here\n",
        "sql_model = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', num_labels=2)\n",
        "other_vuln_model = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', num_labels=other_vuln_data['Vulnerability Status'].nunique())\n",
        "\n",
        "# Setup training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
        ")\n",
        "\n",
        "# Initialize the Trainer for SQL model\n",
        "trainer_sql = Trainer(\n",
        "    model=sql_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_sql,\n",
        "    eval_dataset=val_sql,\n",
        "    compute_metrics=compute_metrics  # Include custom compute_metrics function\n",
        ")\n",
        "trainer_other = Trainer(\n",
        "    model=other_vuln_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_other,\n",
        "    eval_dataset=val_other,\n",
        "    compute_metrics=compute_metrics  # Include custom compute_metrics function\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "CuREemBsPHPn",
        "outputId": "b1043ad2-d9cb-422b-b776-6dbba143e239"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [675/675 05:03, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Description seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Attack Vector seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Description seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Attack Vector seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=675, training_loss=0.07264020845397479, metrics={'train_runtime': 303.7848, 'train_samples_per_second': 8.888, 'train_steps_per_second': 2.222, 'total_flos': 710399849472000.0, 'train_loss': 0.07264020845397479, 'epoch': 3.0})"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Train both models\n",
        "trainer_sql.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "LJP_r5SYYN34",
        "outputId": "b51aaadf-45ef-47c6-c372-cf5186cc7ae9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [675/675 05:06, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.013500</td>\n",
              "      <td>0.006133</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.004900</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.004100</td>\n",
              "      <td>0.001669</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Attack Vector seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.10/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: Description seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=675, training_loss=0.04582816220268055, metrics={'train_runtime': 307.1906, 'train_samples_per_second': 8.789, 'train_steps_per_second': 2.197, 'total_flos': 710393471078400.0, 'train_loss': 0.04582816220268055, 'epoch': 3.0})"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer_other.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "k261DB-E3jjI",
        "outputId": "14306514-a5dd-42ca-c3af-c10049189d22"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 05:45]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13/13 00:02]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SQL Injection Model Results: {'eval_loss': 0.00011795405589509755, 'eval_accuracy': 1.0, 'eval_precision': 1.0, 'eval_recall': 1.0, 'eval_f1': 1.0, 'eval_runtime': 2.8968, 'eval_samples_per_second': 34.521, 'eval_steps_per_second': 4.488, 'epoch': 3.0}\n",
            "General Vulnerabilities Model Results: {'eval_loss': 0.005357217974960804, 'eval_model_preparation_time': 0.0035, 'eval_accuracy': 0.0, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_runtime': 2.8541, 'eval_samples_per_second': 35.037, 'eval_steps_per_second': 4.555, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "# Evaluate both models\n",
        "sql_results = trainer_sql.evaluate()\n",
        "other_results = trainer_other.evaluate()\n",
        "\n",
        "# Print results\n",
        "print(\"SQL Injection Model Results:\", sql_results)\n",
        "print(\"General Vulnerabilities Model Results:\", other_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sc6RWzCSUyuA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi51UFJRg3iS"
      },
      "source": [
        "RESULT of Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OhDGtkqhI-1"
      },
      "source": [
        "RESULT of Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt4jQp1YpJWV"
      },
      "outputs": [],
      "source": [
        "model_path_sql = './model_sql'\n",
        "model_path_other = './model_other'\n",
        "trainer_sql.model.save_pretrained(model_path_sql)\n",
        "trainer_other.model.save_pretrained(model_path_other)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzKm72Tx1aWZ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CF38YYBOpTMs"
      },
      "source": [
        "# Build a Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh8LwnD6rctL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7zTSDb_lL3q"
      },
      "source": [
        "Predicting the the vulnerbility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFBWTG9bPwIV",
        "outputId": "9de8088e-81a6-4bc2-ebbf-7c9cc908b94b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers # Ensure transformers is installed.\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification # Import the RobertaTokenizer and RobertaForSequenceClassification classes\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import torch # Import torch\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n",
        "sql_model = RobertaForSequenceClassification.from_pretrained('/content/model_sql')\n",
        "other_vuln_model = RobertaForSequenceClassification.from_pretrained('/content/model_sql')\n",
        "\n",
        "\n",
        "def scrape_webpage(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    html_content = str(soup)\n",
        "    script_content = [script.text for script in soup.find_all('script') if script.text]\n",
        "    hidden_inputs = soup.find_all('input', {'type': 'hidden'})\n",
        "    data_attributes = [tag for tag in soup.find_all(attrs=re.compile(r'^data-'))]\n",
        "    sql_candidates = []\n",
        "\n",
        "    for input_tag in hidden_inputs:\n",
        "        if 'sql' in input_tag.get('value', '').lower():\n",
        "            sql_candidates.append(input_tag['value'])\n",
        "\n",
        "    for data_tag in data_attributes:\n",
        "        for attribute, value in data_tag.attrs.items():\n",
        "            if 'sql' in attribute.lower() and isinstance(value, str):\n",
        "                sql_candidates.append(value)\n",
        "\n",
        "    return html_content, script_content, sql_candidates\n",
        "\n",
        "\n",
        "def suggest_fixes(sql_prediction, other_vuln_prediction, script_content):\n",
        "  \"\"\"Provides code suggestions based on detected vulnerabilities.\"\"\"\n",
        "  suggestions = []\n",
        "  if sql_prediction == 1:\n",
        "    suggestions.append(\n",
        "        \"**SQL Injection Detected:**\\n\"\n",
        "        \"**Suggestion:** Sanitize user inputs before using them in SQL queries.\\n\"\n",
        "        \"**Example:** Use parameterized queries or escape special characters.\"\n",
        "    )\n",
        "\n",
        "  if other_vuln_prediction == 1:\n",
        "    suggestions.append(\n",
        "        \"**Other Vulnerability (XSS, CSRF, RCE) Detected:**\\n\"\n",
        "        \"**Suggestion:** Investigate the script_content for potential vulnerable code.\\n\"\n",
        "        \"**Possible fixes:**\\n\"\n",
        "        \" - **XSS:** Encode user input before displaying it.\\n\"\n",
        "        \" - **CSRF:** Use anti-CSRF tokens.\\n\"\n",
        "        \" - **RCE:** Validate user input to prevent code execution.\"\n",
        "    )\n",
        "\n",
        "\n",
        "  if not suggestions:\n",
        "    suggestions.append(\"No vulnerabilities detected.\")\n",
        "\n",
        "  return suggestions\n",
        "\n",
        "\n",
        "def predict_and_suggest(url):\n",
        "    \"\"\"Predicts vulnerabilities and suggests fixes for a given URL.\"\"\"\n",
        "    html_content, script_content, sql_candidates = scrape_webpage(url) # scrape_webpage is now defined\n",
        "    encoded_input = tokenizer(script_content, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    sql_outputs = sql_model(**encoded_input)\n",
        "    other_vuln_outputs = other_vuln_model(**encoded_input)\n",
        "    sql_predictions = torch.argmax(sql_outputs.logits, dim=-1)\n",
        "    other_vuln_predictions = torch.argmax(other_vuln_outputs.logits, dim=-1)\n",
        "\n",
        "    suggestions = suggest_fixes(sql_predictions.item(), other_vuln_predictions.item(), script_content)\n",
        "\n",
        "    return suggestions\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "url = \"https://www.facebook.com/\"  # Replace with the target URL\n",
        "suggestions = predict_and_suggest(url)\n",
        "\n",
        "for suggestion in suggestions:\n",
        "  print(suggestion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9em-nM9g1Jq",
        "outputId": "86679d97-bf1d-4b4d-beaf-b173519df3c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers # Ensure transformers is installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 786
        },
        "id": "9lV3Fflbubjx",
        "outputId": "2b28852f-aea6-430b-ad27-fddec542af14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'scrape_webpage' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b7186ded32ee>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Example Usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://www.facebook.com/\"\u001b[0m  \u001b[0;31m# Replace with the target URL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0msuggestions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_and_suggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msuggestion\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuggestions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b7186ded32ee>\u001b[0m in \u001b[0;36mpredict_and_suggest\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_and_suggest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;34m\"\"\"Predicts vulnerabilities and suggests fixes for a given URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mhtml_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscript_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql_candidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscrape_webpage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mencoded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscript_content\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0msql_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msql_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoded_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'scrape_webpage' is not defined"
          ]
        }
      ],
      "source": [
        "!pip install transformers # Ensure transformers is installed.\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification # Import the RobertaTokenizer and RobertaForSequenceClassification classes\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n",
        "sql_model = RobertaForSequenceClassification.from_pretrained('/content/model_sql')\n",
        "other_vuln_model = RobertaForSequenceClassification.from_pretrained('/content/model_sql')\n",
        "\n",
        "\n",
        "def suggest_fixes(sql_prediction, other_vuln_prediction, script_content):\n",
        "  \"\"\"Provides code suggestions based on detected vulnerabilities.\"\"\"\n",
        "  suggestions = []\n",
        "  if sql_prediction == 1:\n",
        "    suggestions.append(\n",
        "        \"**SQL Injection Detected:**\\n\"\n",
        "        \"**Suggestion:** Sanitize user inputs before using them in SQL queries.\\n\"\n",
        "        \"**Example:** Use parameterized queries or escape special characters.\"\n",
        "    )\n",
        "\n",
        "  if other_vuln_prediction == 1:\n",
        "    suggestions.append(\n",
        "        \"**Other Vulnerability (XSS, CSRF, RCE) Detected:**\\n\"\n",
        "        \"**Suggestion:** Investigate the script_content for potential vulnerable code.\\n\"\n",
        "        \"**Possible fixes:**\\n\"\n",
        "        \" - **XSS:** Encode user input before displaying it.\\n\"\n",
        "        \" - **CSRF:** Use anti-CSRF tokens.\\n\"\n",
        "        \" - **RCE:** Validate user input to prevent code execution.\"\n",
        "    )\n",
        "\n",
        "\n",
        "  if not suggestions:\n",
        "    suggestions.append(\"No vulnerabilities detected.\")\n",
        "\n",
        "  return suggestions\n",
        "\n",
        "\n",
        "def predict_and_suggest(url):\n",
        "    \"\"\"Predicts vulnerabilities and suggests fixes for a given URL.\"\"\"\n",
        "    html_content, script_content, sql_candidates = scrape_webpage(url)\n",
        "    encoded_input = tokenizer(script_content, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "    sql_outputs = sql_model(**encoded_input)\n",
        "    other_vuln_outputs = other_vuln_model(**encoded_input)\n",
        "    sql_predictions = torch.argmax(sql_outputs.logits, dim=-1)\n",
        "    other_vuln_predictions = torch.argmax(other_vuln_outputs.logits, dim=-1)\n",
        "\n",
        "    suggestions = suggest_fixes(sql_predictions.item(), other_vuln_predictions.item(), script_content)\n",
        "\n",
        "    return suggestions\n",
        "\n",
        "\n",
        "# Example Usage\n",
        "url = \"https://www.facebook.com/\"  # Replace with the target URL\n",
        "suggestions = predict_and_suggest(url)\n",
        "\n",
        "for suggestion in suggestions:\n",
        "  print(suggestion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7GgvIYJubXK"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5687073,
          "sourceId": 9375764,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5687076,
          "sourceId": 9375768,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}