{"cells":[{"cell_type":"code","execution_count":41,"metadata":{"id":"KZndYfeED6vU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728048676575,"user_tz":-360,"elapsed":4295,"user":{"displayName":"Rezaur Rahman Ratul","userId":"02961116579218275878"}},"outputId":"2528d1a4-71b9-4b5d-a149-6ec103c38887"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n"]},{"cell_type":"markdown","metadata":{"id":"mtIsiqIXg3iI"},"source":["# Step 1: Install and Import Libraries\n"]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3023,"status":"ok","timestamp":1728048686735,"user":{"displayName":"Rezaur Rahman Ratul","userId":"02961116579218275878"},"user_tz":-360},"id":"2asARvczg3iJ","outputId":"caaf1130-0f78-4ca5-d8f3-e56235d48a91"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"]}],"source":["!pip install transformers\n","\n","import pandas as pd\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n"]},{"cell_type":"markdown","metadata":{"id":"eZ6EMCz2g3iK"},"source":["# **Step2: Load SQL Injection and Other Vulnerabilities Datasets**"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":629,"status":"ok","timestamp":1728048689941,"user":{"displayName":"Rezaur Rahman Ratul","userId":"02961116579218275878"},"user_tz":-360},"id":"Tb5WM0Iui58Q","outputId":"1cedd9f1-ea97-4bda-f4db-848fc93ac570"},"outputs":[{"output_type":"stream","name":"stdout","text":["SQL Injection Dataset:\n","      ID                                          SQL Query Injection Payload  \\\n","521  522         UPDATE accounts SET balance=500 WHERE id=1               NaN   \n","737  738  DELETE FROM orders WHERE order_id = john.doe@e...               NaN   \n","740  741         UPDATE accounts SET balance=500 WHERE id=2               NaN   \n","660  661     UPDATE accounts SET balance=500 WHERE id=apple               NaN   \n","411  412  SELECT email FROM customers WHERE email = '' O...           1=1;--'   \n","\n","    Injection Type Vulnerability Status     Injection Result Attack Type  \n","521            NaN                   No                  NaN      Normal  \n","737            NaN                   No                  NaN      Normal  \n","740            NaN                   No                  NaN      Normal  \n","660            NaN                   No                  NaN      Normal  \n","411    Union-based                  Yes  Unauthorized access      Attack  \n","\n","Other Vulnerabilities Dataset:\n","     Unnamed: 0   ID Attack Type                           Attack Vector  \\\n","599         805  806         XSS                 '><svg/onload=alert(1)>   \n","505         681  682         RCE     POST request with command injection   \n","629         844  845        CSRF   Forged request to delete user account   \n","219         303  304        CSRF  Forged request to change user settings   \n","559         756  757        CSRF   Forged request to delete user account   \n","\n","    Vulnerability Status                          Attack Result Request Type  \\\n","599                  Yes                        Script executed          GET   \n","505                  Yes               Arbitrary code execution          GET   \n","629                  Yes  Action performed without user consent          PUT   \n","219                  Yes  Action performed without user consent         POST   \n","559                  Yes  Action performed without user consent       DELETE   \n","\n","                                           Description  \n","599              Reflected or Stored XSS vulnerability  \n","505  Remote Code Execution vulnerability through un...  \n","629           Cross-Site Request Forgery vulnerability  \n","219           Cross-Site Request Forgery vulnerability  \n","559           Cross-Site Request Forgery vulnerability  \n","\n","SQL Injection Dataset Columns: Index(['ID', 'SQL Query', 'Injection Payload', 'Injection Type',\n","       'Vulnerability Status', 'Injection Result', 'Attack Type'],\n","      dtype='object')\n","Other Vulnerabilities Dataset Columns: Index(['Unnamed: 0', 'ID', 'Attack Type', 'Attack Vector',\n","       'Vulnerability Status', 'Attack Result', 'Request Type', 'Description'],\n","      dtype='object')\n"]}],"source":["import pandas as pd\n","from sklearn.utils import shuffle\n","\n","# Load the SQL Injection dataset\n","sql_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Train/sql_injection2.0.csv')\n","# Load the Other Vulnerabilities dataset (for XSS, CSRF, RCE)\n","other_vuln_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Train/Others_vuln2.0.csv')\n","\n","# Shuffle the datasets\n","sql_data_shuffled = shuffle(sql_data)\n","other_vuln_data_shuffled = shuffle(other_vuln_data)\n","\n","# Verify the datasets\n","print(\"SQL Injection Dataset:\")\n","print(sql_data_shuffled.head())\n","\n","print(\"\\nOther Vulnerabilities Dataset:\")\n","print(other_vuln_data_shuffled.head())\n","\n","# Print dataset columns\n","print(\"\\nSQL Injection Dataset Columns:\", sql_data_shuffled.columns)\n","print(\"Other Vulnerabilities Dataset Columns:\", other_vuln_data_shuffled.columns)\n"]},{"cell_type":"markdown","metadata":{"id":"h5EUDONig3iL"},"source":["# Step 3: Tokenize Both Datasets Using CodeBERT"]},{"cell_type":"code","execution_count":44,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1326,"status":"ok","timestamp":1728048694661,"user":{"displayName":"Rezaur Rahman Ratul","userId":"02961116579218275878"},"user_tz":-360},"id":"XGgMObXGg3iM","outputId":"45791c22-75d3-4c2f-bfc3-3fbf5ca39096"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["SQL Encoded Data:\n","Input IDs: torch.Size([1000, 512])\n","Attention Masks: torch.Size([1000, 512])\n","\n","Other Vulnerabilities Encoded Data:\n","Input IDs: torch.Size([738, 512])\n","Attention Masks: torch.Size([738, 512])\n"]}],"source":["from transformers import RobertaTokenizer\n","tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n","\n","# Function to tokenize data from multiple columns and combine input_ids\n","def tokenize_data(dataframe, text_columns):\n","    # Assuming concatenation of texts from different columns for tokenization\n","    concatenated_texts = dataframe[text_columns].apply(lambda x: ' '.join(x.dropna().values), axis=1)\n","    return tokenizer(concatenated_texts.tolist(), padding=\"max_length\", truncation=True, max_length=512, return_tensors='pt')\n","\n","# Columns to tokenize - choose how you concatenate based on your model's capacity to handle input\n","sql_columns = ['SQL Query', 'Vulnerability Status']\n","other_vuln_columns = ['Attack Vector', 'Vulnerability Status']\n","\n","# Tokenize data\n","sql_encoded = tokenize_data(sql_data, sql_columns)\n","other_vuln_encoded = tokenize_data(other_vuln_data, other_vuln_columns)\n","\n","# Printing to verify the outputs\n","print(\"SQL Encoded Data:\")\n","print(f\"Input IDs: {sql_encoded['input_ids'].shape}\")\n","print(f\"Attention Masks: {sql_encoded['attention_mask'].shape}\")\n","\n","print(\"\\nOther Vulnerabilities Encoded Data:\")\n","print(f\"Input IDs: {other_vuln_encoded['input_ids'].shape}\")\n","print(f\"Attention Masks: {other_vuln_encoded['attention_mask'].shape}\")"]},{"cell_type":"markdown","metadata":{"id":"IeAHUfPRg3iN"},"source":["# Step 5: Create a PyTorch Dataset Class and Split Both Datasets for Training and Validation\n","This class converts the tokenized data into a format compatible with PyTorch’s Dataset."]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":660,"status":"ok","timestamp":1728048698892,"user":{"displayName":"Rezaur Rahman Ratul","userId":"02961116579218275878"},"user_tz":-360},"id":"ycoF_G4ig8BQ","outputId":"3a9c84dd-67f3-46a8-c6b6-048c23a4a0d9"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'input_ids': tensor([[    0, 49179,  1009,  ...,     1,     1,     1],\n","        [    0, 41552, 32761,  ...,     1,     1,     1],\n","        [    0, 49179,  1047,  ...,     1,     1,     1],\n","        ...,\n","        [    0, 10089,  3850,  ...,     1,     1,     1],\n","        [    0, 47060,  2069,  ...,     1,     1,     1],\n","        [    0, 34543,  2349,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.])}\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-45-288f762f9fde>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"]}],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n","\n","class CustomDataset(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}  # Correct way to handle tensor copying\n","        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","class CodeDatasetOther(Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = [float(label) for label in labels]\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","# Assuming sql_encoded and other_vuln_encoded are already prepared using your tokenization function\n","sql_labels = [1 if x == 'Yes' else 0 for x in sql_data['Vulnerability Status']]\n","other_labels = [1 if x == 'Yes' else 0 for x in other_vuln_data['Vulnerability Status']]\n","\n","# Create dataset instances\n","sql_dataset = CustomDataset(sql_encoded, sql_labels)\n","other_vuln_dataset = CodeDatasetOther(other_vuln_encoded, other_labels)\n","\n","# Combine the two datasets\n","combined_dataset = ConcatDataset([sql_dataset, other_vuln_dataset])\n","\n","# Define the split sizes\n","train_size = int(0.8 * len(combined_dataset))  # 80% for training\n","val_size = len(combined_dataset) - train_size  # Remaining 20% for validation\n","\n","# Split the combined dataset into training and validation sets\n","train_dataset, val_dataset = random_split(combined_dataset, [train_size, val_size])\n","\n","# Create DataLoader instances for the training and validation sets\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n","\n","# Example loop to illustrate how to handle data loader outputs\n","for batch in train_loader:\n","    print(batch)  # This should print the batches without warnings\n","    break\n","\n","# asasxasxasxxasxa"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":684,"status":"ok","timestamp":1728047227000,"user":{"displayName":"Rezaur Rahman Ratul","userId":"02961116579218275878"},"user_tz":-360},"id":"LIt-hfcag3iN","outputId":"c23aeaf4-6619-408f-b053-6dd036969b4a"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-288f762f9fde>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"]}],"source":["# # Assuming sql_dataset and other_vuln_dataset are your datasets prepared from previous steps\n","# train_sql, val_sql = train_test_split(sql_dataset, test_size=0.2, random_state=42)\n","# train_other, val_other = train_test_split(other_vuln_dataset , test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":625,"status":"ok","timestamp":1728048704280,"user":{"displayName":"Rezaur Rahman Ratul","userId":"02961116579218275878"},"user_tz":-360},"id":"gfeQyKENg3iP","outputId":"5abd202a-768a-4c52-e3ce-8590954a3b95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vulnerability Type Counts:\n"," Attack Type\n","CSRF    273\n","XSS     233\n","RCE     232\n","Name: count, dtype: int64\n"]}],"source":["\n","import pandas as pd\n","sql_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Train/sql_injection2.0.csv')\n","other_vuln_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Train/Others_vuln2.0.csv')\n","vulnerability_counts = other_vuln_data['Attack Type'].value_counts()\n","print(\"Vulnerability Type Counts:\\n\", vulnerability_counts)"]},{"cell_type":"markdown","metadata":{"id":"wjviH_XLg3iP"},"source":["# Step 6: Initialize CodeBERT Models for Both Datasets****"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7qCw6S3Ng3iQ"},"outputs":[],"source":["# # Define model here\n","# model = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', num_labels=2)"]},{"cell_type":"markdown","metadata":{"id":"S1bIToyAg3iR"},"source":["# Train"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2784,"status":"ok","timestamp":1728048748555,"user":{"displayName":"Rezaur Rahman Ratul","userId":"02961116579218275878"},"user_tz":-360},"id":"xtFBttq42Epy","outputId":"11123709-5e4d-4858-f576-56ea18cd0f86"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: seqeval in /usr/local/lib/python3.10/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.26.4)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval) (1.5.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.13.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\n"]}],"source":["!pip install seqeval"]},{"cell_type":"markdown","metadata":{"id":"cOKH_WAPpc5h"},"source":["# IoU Calculation Function"]},{"cell_type":"markdown","metadata":{"id":"1wpDD_jVpgHv"},"source":["# Compute_metrics Function"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"D76iN1m-3WiY","executionInfo":{"status":"ok","timestamp":1728048762370,"user_tz":-360,"elapsed":9,"user":{"displayName":"Rezaur Rahman Ratul","userId":"02961116579218275878"}}},"outputs":[],"source":["from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n","import numpy as np\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","\n","    # Get probabilities for class 1\n","    predictions = predictions[:, 1] # Use probabilities for positive class\n","\n","    # Convert labels to floats\n","    labels = labels.astype(float)\n","\n","    # Calculate precision, recall, and F1-score\n","    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions.round(), average='binary')\n","\n","    # Calculate AUC-ROC\n","    auc_roc = roc_auc_score(labels, predictions)\n","\n","    return {\n","        \"accuracy\": accuracy_score(labels, predictions.round()),\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"f1\": f1,\n","        \"auc_roc\": auc_roc\n","    }"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":504},"id":"juH_KhN22SX1","outputId":"488428ea-8647-4a3d-98b4-b68033220d3a","executionInfo":{"status":"error","timestamp":1728048931306,"user_tz":-360,"elapsed":57221,"user":{"displayName":"Rezaur Rahman Ratul","userId":"02961116579218275878"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-45-288f762f9fde>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='74' max='522' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [ 74/522 00:54 < 05:36, 1.33 it/s, Epoch 0.42/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"RuntimeError","evalue":"Found dtype Long but expected Float","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-f0baf6bb9dc4>\u001b[0m in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# Start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1939\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2278\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2279\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3347\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3348\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3349\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3351\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2196\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    770\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mRuntimeError\u001b[0m: Found dtype Long but expected Float"]}],"source":["from transformers import Trainer, TrainingArguments\n","from transformers import RobertaForSequenceClassification\n","\n","# Define model here for binary classification\n","model = RobertaForSequenceClassification.from_pretrained('microsoft/codebert-base', num_labels=1)  # Change to 1 for binary\n","\n","# Setup training arguments\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=8,\n","    warmup_steps=500,\n","    weight_decay=0.01,\n","    logging_dir='./logs',\n","    logging_steps=10,\n","    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",")\n","\n","# Initialize the Trainer for the combined model\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,  # Use the combined training dataset\n","    eval_dataset=val_dataset,      # Use the combined validation dataset\n","    compute_metrics=compute_metrics  # Using the revised compute_metrics function\n",")\n","\n","# Start training\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256},"id":"CuREemBsPHPn","outputId":"ab640126-36a2-4906-dd05-704c361b1899","executionInfo":{"status":"ok","timestamp":1727861392014,"user_tz":-360,"elapsed":270884,"user":{"displayName":"Rezaur Rahman Ratul","userId":"02961116579218275878"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='339' max='339' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [339/339 04:27, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>Precision</th>\n","      <th>Recall</th>\n","      <th>F1</th>\n","      <th>Auc Roc</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.006600</td>\n","      <td>0.001594</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.000600</td>\n","      <td>0.000222</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.000200</td>\n","      <td>0.000087</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=339, training_loss=0.14586889224640673, metrics={'train_runtime': 269.4661, 'train_samples_per_second': 10.02, 'train_steps_per_second': 1.258, 'total_flos': 710399849472000.0, 'train_loss': 0.14586889224640673, 'epoch': 3.0})"]},"metadata":{},"execution_count":12}],"source":["# # Train both models\n","# trainer_.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LJP_r5SYYN34","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"error","timestamp":1727861398677,"user_tz":-360,"elapsed":771,"user":{"displayName":"Rezaur Rahman Ratul","userId":"02961116579218275878"}},"outputId":"c7bb905e-4f23-48ca-cdc2-c1f34de137b6"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 2]))","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-6b0bc84df9ba>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer_other\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1937\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1938\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1939\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1940\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2278\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2279\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3317\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3318\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3320\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3361\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1230\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1232\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         return F.binary_cross_entropy_with_logits(input, target,\n\u001b[0m\u001b[1;32m    735\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3242\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Target size ({target.size()}) must be the same as input size ({input.size()})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3244\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([8])) must be the same as input size (torch.Size([8, 2]))"]}],"source":["# trainer_other.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MATFnTB5ac21"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sc6RWzCSUyuA"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"fi51UFJRg3iS"},"source":["RESULT of Train"]},{"cell_type":"markdown","metadata":{"id":"-OhDGtkqhI-1"},"source":["RESULT of Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zt4jQp1YpJWV"},"outputs":[],"source":["model_path_sql = './model_sql'\n","model_path_other = './model_other'\n","trainer_sql.model.save_pretrained(model_path_sql)\n","trainer_other.model.save_pretrained(model_path_other)"]},{"cell_type":"markdown","metadata":{"id":"gzKm72Tx1aWZ"},"source":[]},{"cell_type":"markdown","metadata":{"id":"CF38YYBOpTMs"},"source":["# Build a Scraping"]},{"cell_type":"markdown","metadata":{"id":"yh8LwnD6rctL"},"source":[]},{"cell_type":"markdown","metadata":{"id":"x7zTSDb_lL3q"},"source":["Predicting the the vulnerbility"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFBWTG9bPwIV"},"outputs":[],"source":["!pip install transformers # Ensure transformers is installed.\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification # Import the RobertaTokenizer and RobertaForSequenceClassification classes\n","import requests\n","from bs4 import BeautifulSoup\n","import re\n","import torch # Import torch\n","\n","# Load tokenizer and model\n","tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n","sql_model = RobertaForSequenceClassification.from_pretrained('/content/model_sql')\n","other_vuln_model = RobertaForSequenceClassification.from_pretrained('/content/model_sql')\n","\n","\n","def scrape_webpage(url):\n","    response = requests.get(url)\n","    soup = BeautifulSoup(response.text, 'html.parser')\n","\n","    html_content = str(soup)\n","    script_content = [script.text for script in soup.find_all('script') if script.text]\n","    hidden_inputs = soup.find_all('input', {'type': 'hidden'})\n","    data_attributes = [tag for tag in soup.find_all(attrs=re.compile(r'^data-'))]\n","    sql_candidates = []\n","\n","    for input_tag in hidden_inputs:\n","        if 'sql' in input_tag.get('value', '').lower():\n","            sql_candidates.append(input_tag['value'])\n","\n","    for data_tag in data_attributes:\n","        for attribute, value in data_tag.attrs.items():\n","            if 'sql' in attribute.lower() and isinstance(value, str):\n","                sql_candidates.append(value)\n","\n","    return html_content, script_content, sql_candidates\n","\n","\n","def suggest_fixes(sql_prediction, other_vuln_prediction, script_content):\n","  \"\"\"Provides code suggestions based on detected vulnerabilities.\"\"\"\n","  suggestions = []\n","  if sql_prediction == 1:\n","    suggestions.append(\n","        \"**SQL Injection Detected:**\\n\"\n","        \"**Suggestion:** Sanitize user inputs before using them in SQL queries.\\n\"\n","        \"**Example:** Use parameterized queries or escape special characters.\"\n","    )\n","\n","  if other_vuln_prediction == 1:\n","    suggestions.append(\n","        \"**Other Vulnerability (XSS, CSRF, RCE) Detected:**\\n\"\n","        \"**Suggestion:** Investigate the script_content for potential vulnerable code.\\n\"\n","        \"**Possible fixes:**\\n\"\n","        \" - **XSS:** Encode user input before displaying it.\\n\"\n","        \" - **CSRF:** Use anti-CSRF tokens.\\n\"\n","        \" - **RCE:** Validate user input to prevent code execution.\"\n","    )\n","\n","\n","  if not suggestions:\n","    suggestions.append(\"No vulnerabilities detected.\")\n","\n","  return suggestions\n","\n","\n","def predict_and_suggest(url):\n","    \"\"\"Predicts vulnerabilities and suggests fixes for a given URL.\"\"\"\n","    html_content, script_content, sql_candidates = scrape_webpage(url) # scrape_webpage is now defined\n","    encoded_input = tokenizer(script_content, return_tensors=\"pt\", truncation=True, padding=True)\n","    sql_outputs = sql_model(**encoded_input)\n","    other_vuln_outputs = other_vuln_model(**encoded_input)\n","    sql_predictions = torch.argmax(sql_outputs.logits, dim=-1)\n","    other_vuln_predictions = torch.argmax(other_vuln_outputs.logits, dim=-1)\n","\n","    suggestions = suggest_fixes(sql_predictions.item(), other_vuln_predictions.item(), script_content)\n","\n","    return suggestions\n","\n","\n","# Example Usage\n","url = \"https://www.facebook.com/\"  # Replace with the target URL\n","suggestions = predict_and_suggest(url)\n","\n","for suggestion in suggestions:\n","  print(suggestion)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9em-nM9g1Jq"},"outputs":[],"source":["!pip install transformers # Ensure transformers is installed."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9lV3Fflbubjx"},"outputs":[],"source":["!pip install transformers # Ensure transformers is installed.\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification # Import the RobertaTokenizer and RobertaForSequenceClassification classes\n","\n","# Load tokenizer and model\n","tokenizer = RobertaTokenizer.from_pretrained('microsoft/codebert-base')\n","sql_model = RobertaForSequenceClassification.from_pretrained('/content/model_sql')\n","other_vuln_model = RobertaForSequenceClassification.from_pretrained('/content/model_sql')\n","\n","\n","def suggest_fixes(sql_prediction, other_vuln_prediction, script_content):\n","  \"\"\"Provides code suggestions based on detected vulnerabilities.\"\"\"\n","  suggestions = []\n","  if sql_prediction == 1:\n","    suggestions.append(\n","        \"**SQL Injection Detected:**\\n\"\n","        \"**Suggestion:** Sanitize user inputs before using them in SQL queries.\\n\"\n","        \"**Example:** Use parameterized queries or escape special characters.\"\n","    )\n","\n","  if other_vuln_prediction == 1:\n","    suggestions.append(\n","        \"**Other Vulnerability (XSS, CSRF, RCE) Detected:**\\n\"\n","        \"**Suggestion:** Investigate the script_content for potential vulnerable code.\\n\"\n","        \"**Possible fixes:**\\n\"\n","        \" - **XSS:** Encode user input before displaying it.\\n\"\n","        \" - **CSRF:** Use anti-CSRF tokens.\\n\"\n","        \" - **RCE:** Validate user input to prevent code execution.\"\n","    )\n","\n","\n","  if not suggestions:\n","    suggestions.append(\"No vulnerabilities detected.\")\n","\n","  return suggestions\n","\n","\n","def predict_and_suggest(url):\n","    \"\"\"Predicts vulnerabilities and suggests fixes for a given URL.\"\"\"\n","    html_content, script_content, sql_candidates = scrape_webpage(url)\n","    encoded_input = tokenizer(script_content, return_tensors=\"pt\", truncation=True, padding=True)\n","    sql_outputs = sql_model(**encoded_input)\n","    other_vuln_outputs = other_vuln_model(**encoded_input)\n","    sql_predictions = torch.argmax(sql_outputs.logits, dim=-1)\n","    other_vuln_predictions = torch.argmax(other_vuln_outputs.logits, dim=-1)\n","\n","    suggestions = suggest_fixes(sql_predictions.item(), other_vuln_predictions.item(), script_content)\n","\n","    return suggestions\n","\n","\n","# Example Usage\n","url = \"https://www.facebook.com/\"  # Replace with the target URL\n","suggestions = predict_and_suggest(url)\n","\n","for suggestion in suggestions:\n","  print(suggestion)"]},{"cell_type":"markdown","metadata":{"id":"W7GgvIYJubXK"},"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1eskFpY-96a62HTzLMy6ube0vFQVB195G","timestamp":1728049026768},{"file_id":"1RyUyrQEPHScrsp3fnd1RfPCm2OFhkrgV","timestamp":1727861463826},{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/notebook3e3f0107d6-aa7d99a7-71f1-4f3e-9221-e111841da202.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20240925/auto/storage/goog4_request&X-Goog-Date=20240925T024025Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=dd3b52f28fb657782cb7e517753dba8653d814ed4e2c5b6de6714d12f31dd9b98c39dd33c42438977403f48e19d98ab039da325d11b02b7eb0bb3efdcf51d138aa4a7f80acafd27c1304ac3e314325c288b5078dab432f9d01b4347e235ece4d80e4b365130a3814a3df6d7d74da3cd9e79ce2a0814afc7b49d9c1c5cbc5f814cdf977bac3adc8ee4fa258096b4e5da1b87bc5b2409dd12eaa3f56a4d5ba34acaccfd971f2a8b39583f9f78b2641e1e32480e3b354e393c01e8586b140d88d9675f172525bed5297674b33e323c9ae8e863e70b753df5b20e035a5e2139c318df8f0a887b321a2cff3cfc14391c8aa3d14dcbdbd5af60960e0368a8799c7e160","timestamp":1727232644217}],"gpuType":"T4"},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5687073,"sourceId":9375764,"sourceType":"datasetVersion"},{"datasetId":5687076,"sourceId":9375768,"sourceType":"datasetVersion"}],"dockerImageVersionId":30776,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}